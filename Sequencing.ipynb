{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7341d94",
   "metadata": {},
   "source": [
    "# Takes .qual File and Generates the Corresponding CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe635c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file = r\"D:\\Malak\\iped\\validation\\filtered_mockfb_clean.qual\"\n",
    "output_file = r\"D:\\Malak\\iped\\validation\\mockb_data_qual.csv\"\n",
    "\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"Sequence_ID\", \"Quality_Scores\"])\n",
    "\n",
    "    seq_id = None\n",
    "    scores = []\n",
    "\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\">\"):\n",
    "            if seq_id is not None:\n",
    "                writer.writerow([seq_id, \" \".join(scores)])\n",
    "            seq_id = line[1:]  # remove \">\"\n",
    "            scores = []\n",
    "        else:\n",
    "            scores.extend(line.split())\n",
    "\n",
    "    # Write the last sequence\n",
    "    if seq_id is not None:\n",
    "        writer.writerow([seq_id, \" \".join(scores)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a2557",
   "metadata": {},
   "source": [
    "# Takes .error File and Generates the Corresponding CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file = r\"D:\\Malak\\iped\\validation\\filtered_mockfb_clean.error\"\n",
    "output_file = r\"D:\\Malak\\iped\\validation\\mockb_sequences.csv\"\n",
    "\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"Sequence_ID2\", \"Sequence\"])\n",
    "\n",
    "    seq_id = \"\"\n",
    "    sequence = \"\"\n",
    "\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\">\"):\n",
    "            if seq_id:\n",
    "                writer.writerow([seq_id, sequence])\n",
    "            seq_id = line[1:]  # remove '>'\n",
    "            sequence = \"\"\n",
    "        else:\n",
    "            sequence += line\n",
    "\n",
    "    # Write last sequence\n",
    "    if seq_id:\n",
    "        writer.writerow([seq_id, sequence])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6610088",
   "metadata": {},
   "source": [
    "# Merge the 2 Generated CSVs to a Single File with 3 Columns (ID, Sequence, Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your file\n",
    "df = pd.read_csv(r\"D:\\Malak\\iped\\validation\\data.csv\")  # replace with actual filename\n",
    "\n",
    "# Clean Sequence_ID2: remove \" ref:...\" to get the comparable ID\n",
    "df[\"cleaned_ID2\"] = df[\"Sequence_ID2\"].str.extract(r\"^([^\\s]+)\")\n",
    "\n",
    "# Create a mapping: cleaned_ID2 → Sequence\n",
    "seq_dict = df.set_index(\"cleaned_ID2\")[\"Sequence\"].to_dict()\n",
    "\n",
    "# Now build result: use Sequence_ID and Quality_Scores directly\n",
    "# Use Sequence_ID to look up matching Sequence from Sequence_ID2 side\n",
    "df[\"matched_sequence\"] = df[\"Sequence_ID\"].map(seq_dict)\n",
    "\n",
    "# Prepare final result\n",
    "final = df[[\"Sequence_ID\", \"Quality_Scores\", \"matched_sequence\"]].dropna()\n",
    "final.columns = [\"Sequence_ID\", \"Quality_Scores\", \"Sequence\"]\n",
    "\n",
    "# Save to CSV\n",
    "final.to_csv(r\"D:\\Malak\\iped\\validation\\final_matched_output.csv\", index=False)\n",
    "\n",
    "print(\"✅ Matching completed and saved to 'final_matched_output.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee1812",
   "metadata": {},
   "source": [
    "# Process the File, Separate Bases, and Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d42ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def compute_window_abundance(base_seq, window_size=21):\n",
    "    counts = Counter()\n",
    "    half = window_size // 2\n",
    "    padded_seq = ['X'] * half + base_seq + ['X'] * half\n",
    "    for i in range(len(base_seq)):\n",
    "        window = ''.join(padded_seq[i:i+window_size])\n",
    "        counts[window] += 1\n",
    "    return counts\n",
    "\n",
    "def assign_window_abundance(base_seq, abundance_dict, window_size=21):\n",
    "    half = window_size // 2\n",
    "    padded_seq = ['X'] * half + base_seq + ['X'] * half\n",
    "    abundances = []\n",
    "    for i in range(len(base_seq)):\n",
    "        window = ''.join(padded_seq[i:i+window_size])\n",
    "        abundances.append(abundance_dict.get(window, 0))\n",
    "    return abundances\n",
    "\n",
    "def parse_contig_data(contig_str, matching_string, contig_index):\n",
    "    contig_str = contig_str.replace('-', '.')\n",
    "    entries = contig_str.strip().split()\n",
    "    parsed = []\n",
    "    for entry in entries:\n",
    "        if re.match(r'^\\d+[ACGT]{1}\\.0$', entry):  # Forward-only\n",
    "            phred = int(entry[:-3])\n",
    "            base = entry[-3]\n",
    "            parsed.append({'region': 'forward', 'base': base, 'forward_phred': phred, 'reverse_phred': None})\n",
    "        elif re.match(r'^\\d+[ACGT]{1,2}\\d+$', entry):  # Overlap\n",
    "            m = re.match(r'^(\\d+)([ACGT]{1,2})(\\d+)$', entry)\n",
    "            f_phred, base, r_phred = int(m[1]), m[2], int(m[3])\n",
    "            parsed.append({'region': 'overlap', 'base': base, 'forward_phred': f_phred, 'reverse_phred': r_phred})\n",
    "        elif re.match(r'^0\\.[ACGT]\\d+$', entry):  # Reverse-only\n",
    "            base = entry[2]\n",
    "            r_phred = int(entry[3:])\n",
    "            parsed.append({'region': 'reverse', 'base': base, 'forward_phred': None, 'reverse_phred': r_phred})\n",
    "        else:\n",
    "            print(f\"⚠️ Unknown format in contig {contig_index}: {entry}\")\n",
    "    if len(parsed) != len(matching_string):\n",
    "        print(f\"❌ Contig {contig_index}: Length mismatch ({len(parsed)} vs {len(matching_string)})\")\n",
    "        return []\n",
    "    for i, letter in enumerate(matching_string):\n",
    "        parsed[i]['match_letter'] = letter\n",
    "    return parsed\n",
    "\n",
    "def normalize_base(base):\n",
    "    return base.replace('GG', 'G').replace('AA', 'A').replace('TT', 'T').replace('CC', 'C')\n",
    "\n",
    "def assign_homopolymer_positions(base_list, reverse=False):\n",
    "    result = []\n",
    "    if reverse:\n",
    "        base_list = base_list[::-1]\n",
    "    for i in range(len(base_list)):\n",
    "        curr = base_list[i]\n",
    "        prev = base_list[i - 1] if i - 1 >= 0 else None\n",
    "        prev2 = base_list[i - 2] if i - 2 >= 0 else None\n",
    "        prev3 = base_list[i - 3] if i - 3 >= 0 else None\n",
    "        next_ = base_list[i + 1] if i + 1 < len(base_list) else None\n",
    "        if curr != prev and curr != next_:\n",
    "            pos = \"solo\"\n",
    "        elif curr != prev and curr == next_:\n",
    "            pos = \"first\"\n",
    "        elif curr == prev and curr != prev2:\n",
    "            pos = \"second\"\n",
    "        elif curr == prev == prev2 and curr != prev3:\n",
    "            pos = \"third\"\n",
    "        else:\n",
    "            count = 0\n",
    "            for j in range(i - 1, -1, -1):\n",
    "                if base_list[j] == curr:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            pos = f\"{count+1}th\"\n",
    "        result.append(pos)\n",
    "    if reverse:\n",
    "        result = result[::-1]\n",
    "    return result\n",
    "\n",
    "def count_homopolymer_length(base_list):\n",
    "    lengths = []\n",
    "    count = 1\n",
    "    for i in range(len(base_list)):\n",
    "        if i == 0:\n",
    "            lengths.append(1)\n",
    "        else:\n",
    "            if base_list[i] == base_list[i - 1]:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "            lengths.append(count)\n",
    "    return lengths\n",
    "\n",
    "def detect_gcc_motif(base_list, reverse=False):\n",
    "    motif_flags = []\n",
    "    if reverse:\n",
    "        base_list = base_list[::-1]\n",
    "    for i in range(len(base_list)):\n",
    "        if i >= 3 and ''.join(base_list[i - 3:i]) == 'GCC':\n",
    "            motif_flags.append(1)\n",
    "        else:\n",
    "            motif_flags.append(0)\n",
    "    if reverse:\n",
    "        motif_flags = motif_flags[::-1]\n",
    "    return motif_flags\n",
    "\n",
    "def compute_gc_and_entropy(seq, window=5):\n",
    "    half = window // 2\n",
    "    gc_list = []\n",
    "    entropy_list = []\n",
    "    for i in range(len(seq)):\n",
    "        start = max(0, i - half)\n",
    "        end = min(len(seq), i + half + 1)\n",
    "        window_seq = seq[start:end]\n",
    "        gc = sum(1 for b in window_seq if b in ['G', 'C']) / len(window_seq)\n",
    "        freqs = Counter(window_seq)\n",
    "        entropy = -sum((v / len(window_seq)) * math.log2(v / len(window_seq)) for v in freqs.values())\n",
    "        gc_list.append(gc)\n",
    "        entropy_list.append(entropy)\n",
    "    return gc_list, entropy_list\n",
    "\n",
    "# === Main function ===\n",
    "def process_all_sequences(input_csv, output_csv, window=5):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    header_written = False\n",
    "\n",
    "    # === Precompute global abundance of full sequences ===\n",
    "    global_sequence_counts = df[\"Sequence\"].value_counts().to_dict()\n",
    "\n",
    "    with open(output_csv, mode='w', newline='') as f_out:\n",
    "        writer = None\n",
    "\n",
    "        forward_risk_map = {\n",
    "            'solo': 0.013018963, 'first': 0.0102533, 'second': 0.008364577, 'third': 0.010356342,\n",
    "            '4th': 0.025978121, '5th': 0.00977431, '6th': 0.012274475, '7th': 0.119402985, '8th': 0.0\n",
    "        }\n",
    "        reverse_risk_map = {\n",
    "            'solo': 0.013018963, 'first': 0.008704605, 'second': 0.00988321, 'third': 0.013463661,\n",
    "            '4th': 0.013611362, '5th': 0.007787314, '6th': 0.003812876, '7th': 0.229508197, '8th': 0.0\n",
    "        }\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            contig_str = str(row['Quality_Scores'])\n",
    "            matching_str = str(row['Sequence']).strip()\n",
    "\n",
    "            parsed = parse_contig_data(contig_str, matching_str, idx)\n",
    "            if not parsed:\n",
    "                continue\n",
    "\n",
    "            for p in parsed:\n",
    "                p['base'] = normalize_base(p['base'])\n",
    "\n",
    "            base_seq = [p['base'] for p in parsed]\n",
    "\n",
    "            # Feature: window abundance\n",
    "            window_abundance_map = compute_window_abundance(base_seq, window_size=21)\n",
    "            window_abundances = assign_window_abundance(base_seq, window_abundance_map, window_size=21)\n",
    "\n",
    "            # Feature: global + local sequence abundance\n",
    "            global_abundance = global_sequence_counts.get(matching_str, 1)\n",
    "            local_abundance = contig_str.strip().split().count(contig_str.strip().split()[0])  # crude: use first token type count\n",
    "\n",
    "            hp_forward = assign_homopolymer_positions(base_seq, reverse=False)\n",
    "            hp_reverse = assign_homopolymer_positions(base_seq, reverse=True)\n",
    "            motif_forward = detect_gcc_motif(base_seq, reverse=False)\n",
    "            motif_reverse = detect_gcc_motif(base_seq, reverse=True)\n",
    "\n",
    "            total_len = len(parsed)\n",
    "            forward_phred = [p['forward_phred'] if p['forward_phred'] is not None else 0 for p in parsed]\n",
    "            reverse_phred = [p['reverse_phred'] if p['reverse_phred'] is not None else 0 for p in parsed]\n",
    "            phred_delta_forward = [0] + [forward_phred[i] - forward_phred[i - 1] for i in range(1, total_len)]\n",
    "            phred_delta_reverse = [0] + [reverse_phred[i] - reverse_phred[i - 1] for i in range(1, total_len)]\n",
    "\n",
    "            hp_len_fwd = count_homopolymer_length(base_seq)\n",
    "            hp_len_rev = count_homopolymer_length(base_seq[::-1])[::-1]\n",
    "            gc_forward, entropy_forward = compute_gc_and_entropy(base_seq, window)\n",
    "            gc_reverse, entropy_reverse = compute_gc_and_entropy(base_seq[::-1], window)\n",
    "            gc_reverse = gc_reverse[::-1]\n",
    "            entropy_reverse = entropy_reverse[::-1]\n",
    "\n",
    "            for i in range(total_len):\n",
    "                parsed[i]['normalized_position_forward'] = i / (total_len - 1) if total_len > 1 else 0.0\n",
    "                parsed[i]['normalized_position_reverse'] = (total_len - 1 - i) / (total_len - 1) if total_len > 1 else 0.0\n",
    "                parsed[i]['homopolymer_position_forward'] = hp_forward[i]\n",
    "                parsed[i]['homopolymer_position_reverse'] = hp_reverse[i]\n",
    "                parsed[i]['homopolymer_error_risk_forward'] = forward_risk_map.get(hp_forward[i], 0)\n",
    "                parsed[i]['homopolymer_error_risk_reverse'] = reverse_risk_map.get(hp_reverse[i], 0)\n",
    "                parsed[i]['homopolymer_length_forward'] = hp_len_fwd[i]\n",
    "                parsed[i]['homopolymer_length_reverse'] = hp_len_rev[i]\n",
    "                parsed[i]['GC_content_forward'] = gc_forward[i]\n",
    "                parsed[i]['GC_content_reverse'] = gc_reverse[i]\n",
    "                parsed[i]['entropy_forward'] = entropy_forward[i]\n",
    "                parsed[i]['entropy_reverse'] = entropy_reverse[i]\n",
    "                parsed[i]['phred_delta_forward'] = phred_delta_forward[i]\n",
    "                parsed[i]['phred_delta_reverse'] = phred_delta_reverse[i]\n",
    "                parsed[i]['is_overlap'] = int(parsed[i]['region'] == 'overlap')\n",
    "                parsed[i]['has_GCC_prev3_forward'] = motif_forward[i]\n",
    "                parsed[i]['has_GCC_prev3_reverse'] = motif_reverse[i]\n",
    "                parsed[i]['is_last_in_contig'] = int(i == len(parsed) - 1)\n",
    "                parsed[i]['window_abundance'] = window_abundances[i]\n",
    "                parsed[i]['global_abundance'] = global_abundance\n",
    "                parsed[i]['local_abundance'] = len(parsed)\n",
    "\n",
    "            if not header_written:\n",
    "                writer = csv.DictWriter(f_out, fieldnames=parsed[0].keys())\n",
    "                writer.writeheader()\n",
    "                header_written = True\n",
    "\n",
    "            writer.writerows(parsed)\n",
    "\n",
    "    print(\"✅ File saved:\", output_csv)\n",
    "    # === Run it ===\n",
    "process_all_sequences(\n",
    "    input_csv=r\"D:\\Malak\\iped\\validation\\final_matched_output.csv\",\n",
    "    output_csv=r\"D:\\Malak\\iped\\validation\\final_parsed_with_motifs.csv\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5d24c",
   "metadata": {},
   "source": [
    "# Extract Features of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90deb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = r\"D:\\Malak\\iped\\validation\\final_parsed_with_motifs.csv\"\n",
    "output_path = r\"D:\\Malak\\iped\\validation\\ml_ready_dataset.csv\"\n",
    "\n",
    "columns_to_keep = [\n",
    "    'homopolymer_length_forward',\n",
    "    'homopolymer_length_reverse',\n",
    "    'normalized_position_forward',\n",
    "    'normalized_position_reverse',\n",
    "    'forward_phred',\n",
    "    'reverse_phred',\n",
    "    'homopolymer_error_risk_forward',\n",
    "    'homopolymer_error_risk_reverse',\n",
    "    'is_overlap',\n",
    "    'has_GCC_prev3_forward',\n",
    "    'has_GCC_prev3_reverse',\n",
    "    'match_letter',\n",
    "    'GC_content_forward',\n",
    "    'GC_content_reverse',\n",
    "    'entropy_forward',\n",
    "    'entropy_reverse',\n",
    "    'window_abundance',\n",
    "    'global_abundance',\n",
    "    'local_abundance'\n",
    "]\n",
    "\n",
    "# Process in chunks\n",
    "chunk_size = 100_000\n",
    "first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(input_path, chunksize=chunk_size, usecols=columns_to_keep):\n",
    "    chunk['match_letter'] = chunk['match_letter'].replace(\"i\", \"s\")\n",
    "    \n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_path, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_path, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"✅ Processed file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984b109",
   "metadata": {},
   "source": [
    "# RF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105fd77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "input_path = r\"D:\\Malak\\iped\\ml_ready_dataset.csv\"\n",
    "balanced_output_path = r\"D:\\Malak\\iped\\balanced_subset.csv\"\n",
    "\n",
    "# === 1. Process in chunks and build balanced subset ===\n",
    "chunk_size = 100_000\n",
    "minority_rows = []\n",
    "majority_rows = []\n",
    "\n",
    "for chunk in pd.read_csv(input_path, chunksize=chunk_size):\n",
    "    chunk[\"label\"] = chunk[\"match_letter\"].map({\"m\": 0, \"s\": 1})\n",
    "    chunk.drop(columns=[\"match_letter\"], inplace=True)\n",
    "\n",
    "    minority = chunk[chunk[\"label\"] == 1]\n",
    "    majority = chunk[chunk[\"label\"] == 0]\n",
    "\n",
    "    minority_rows.append(minority)\n",
    "    if len(minority) > 0:\n",
    "        downsampled_majority = resample(\n",
    "            majority,\n",
    "            replace=False,\n",
    "            n_samples=len(minority) * 5,\n",
    "            random_state=42\n",
    "        )\n",
    "        majority_rows.append(downsampled_majority)\n",
    "\n",
    "# Concatenate and save balanced set\n",
    "df_minority = pd.concat(minority_rows, ignore_index=True)\n",
    "df_majority = pd.concat(majority_rows, ignore_index=True)\n",
    "df_balanced = pd.concat([df_minority, df_majority], ignore_index=True)\n",
    "df_balanced.to_csv(balanced_output_path, index=False)\n",
    "print(f\"✅ Balanced subset saved: {balanced_output_path}\")\n",
    "\n",
    "# === 2. Load small balanced file ===\n",
    "df = pd.read_csv(balanced_output_path)\n",
    "X = df.drop(columns=[\"label\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === 3. Train model ===\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === 4. Threshold evaluation ===\n",
    "thresholds = [0.8,0.7,0.6,0.59,0.58,0.57,0.56,0.55,0.54,0.53,0.52,0.51,0.50,0.49,0.48,0.47,0.46,0.45,0.44,0.43,0.42,0.41, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10]\n",
    "\n",
    "print(\"=== Threshold Evaluation Report ===\")\n",
    "print(\"Threshold | Accuracy | Sensitivity | Specificity\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "    print(f\"{threshold:9.2f} | {acc:.4f}   | {sensitivity:.4f}     | {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1518eb",
   "metadata": {},
   "source": [
    "# Voting Combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# === Load Data ===\n",
    "balanced_output_path = r\"D:\\Malak\\iped\\balanced_subset.csv\"\n",
    "df = pd.read_csv(balanced_output_path)\n",
    "\n",
    "# Impute missing values (mean for numeric)\n",
    "X = df.drop(columns=[\"label\"])\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# === Train Models ===\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "\n",
    "print(\"⏳ Training models...\")\n",
    "rf.fit(X_train, y_train)\n",
    "mlp.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# === Get Probabilities ===\n",
    "rf_proba = rf.predict_proba(X_test)[:, 1]\n",
    "mlp_proba = mlp.predict_proba(X_test)[:, 1]\n",
    "xgb_proba = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Voting Combinations ===\n",
    "model_names = ['RF', 'MLP', 'XGB']\n",
    "probas = {'RF': rf_proba, 'MLP': mlp_proba, 'XGB': xgb_proba}\n",
    "combinations = [combo for i in range(1, 4) for combo in itertools.combinations(model_names, i)]\n",
    "\n",
    "thresholds = [0.8, 0.7, 0.6, 0.59, 0.58, 0.57, 0.56, 0.55, 0.54, 0.53, 0.52,\n",
    "              0.51, 0.50, 0.49, 0.48, 0.47, 0.46, 0.45, 0.44, 0.43, 0.42,\n",
    "              0.41, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10]\n",
    "\n",
    "# === Evaluate Each Combination ===\n",
    "print(\"\\n=== Voting Ensemble Threshold Evaluation ===\")\n",
    "for combo in combinations:\n",
    "    avg_proba = np.mean([probas[model] for model in combo], axis=0)\n",
    "    print(f\"\\n🧠 Combo: {' + '.join(combo)}\")\n",
    "    print(\"Threshold | Accuracy | Sensitivity | Specificity\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (avg_proba >= threshold).astype(int)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "        print(f\"{threshold:9.2f} | {acc:.4f}   | {sensitivity:.4f}     | {specificity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255dccce",
   "metadata": {},
   "source": [
    "# Generate Models (RF + XGB) and Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceba04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# === Config ===\n",
    "input_path = r\"D:\\Malak\\iped\\ml_ready_dataset.csv\"\n",
    "chunk_size = 1_000_000\n",
    "threshold = 0.48\n",
    "balance_ratio = 4  # 1 error : 3 correct\n",
    "\n",
    "# === Prepare growing training set ===\n",
    "balanced_chunks = []\n",
    "\n",
    "print(\"🔄 Building balanced training set...\")\n",
    "for chunk in pd.read_csv(input_path, chunksize=chunk_size):\n",
    "    chunk[\"label\"] = chunk[\"match_letter\"].map({\"m\": 0, \"s\": 1})\n",
    "    chunk.drop(columns=[\"match_letter\"], inplace=True)\n",
    "\n",
    "    # Get all errors and sampled corrects\n",
    "    errors = chunk[chunk[\"label\"] == 1]\n",
    "    corrects = chunk[chunk[\"label\"] == 0].sample(\n",
    "        n=min(len(errors) * balance_ratio, len(chunk[chunk[\"label\"] == 0])),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    balanced_chunk = pd.concat([errors, corrects])\n",
    "    balanced_chunks.append(balanced_chunk)\n",
    "\n",
    "# Combine all balanced chunks\n",
    "balanced_df = pd.concat(balanced_chunks).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"✅ Final balanced training set: {balanced_df.shape}\")\n",
    "\n",
    "# === Prepare training data ===\n",
    "X_train = balanced_df.drop(columns=[\"label\"])\n",
    "y_train = balanced_df[\"label\"]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "# Train models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "\n",
    "print(\"⏳ Training models on full balanced data...\")\n",
    "rf.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# === Load full dataset for evaluation ===\n",
    "print(\"\\n🔍 Evaluating on full dataset...\")\n",
    "\n",
    "full_df = pd.read_csv(input_path)\n",
    "full_df[\"label\"] = full_df[\"match_letter\"].map({\"m\": 0, \"s\": 1})\n",
    "full_df.drop(columns=[\"match_letter\"], inplace=True)\n",
    "full_df = full_df.dropna(subset=[\"label\"])\n",
    "\n",
    "X_full = full_df.drop(columns=[\"label\"])\n",
    "y_full = full_df[\"label\"].values\n",
    "\n",
    "# Impute missing values\n",
    "X_full_imputed = pd.DataFrame(imputer.transform(X_full), columns=X_full.columns)\n",
    "\n",
    "# Get predictions\n",
    "rf_proba = rf.predict_proba(X_full_imputed)[:, 1]\n",
    "xgb_proba = xgb.predict_proba(X_full_imputed)[:, 1]\n",
    "avg_proba = (rf_proba + xgb_proba) / 2\n",
    "y_pred = (avg_proba >= threshold).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "cm = confusion_matrix(y_full, y_pred, labels=[0, 1])\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "acc = accuracy_score(y_full, y_pred)\n",
    "sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Final Evaluation at Threshold 0.46 ===\")\n",
    "print(f\"Samples      : {len(y_full)}\")\n",
    "print(f\"Accuracy     : {acc:.4f}\")\n",
    "print(f\"Sensitivity  : {sensitivity:.4f}\")\n",
    "print(f\"Specificity  : {specificity:.4f}\")\n",
    "print(f\"TP = {TP}, FP = {FP}, TN = {TN}, FN = {FN}\")\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# === Save models and imputer ===\n",
    "model_dir = r\"D:\\Malak\\iped\\models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(rf, os.path.join(model_dir, \"rf_model.pkl\"))\n",
    "joblib.dump(xgb, os.path.join(model_dir, \"xgb_model.pkl\"))\n",
    "joblib.dump(imputer, os.path.join(model_dir, \"imputer.pkl\"))\n",
    "\n",
    "print(f\"\\n💾 Models saved to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81482368",
   "metadata": {},
   "source": [
    "# Test on New Mock Using Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "model_dir = r\"D:\\Malak\\iped\\models\"\n",
    "feature_file = r\"D:\\Malak\\iped\\validation\\ml_ready_dataset.csv\"\n",
    "output_file = r\"D:\\Malak\\iped\\validation\\validation_predictions_for_comparison.csv\"\n",
    "threshold = 0.46\n",
    "\n",
    "# === Step 1: Load models and imputer ===\n",
    "rf = joblib.load(os.path.join(model_dir, \"rf_model.pkl\"))\n",
    "xgb = joblib.load(os.path.join(model_dir, \"xgb_model.pkl\"))\n",
    "imputer = joblib.load(os.path.join(model_dir, \"imputer.pkl\"))\n",
    "\n",
    "# === Step 2: Define the same features as in training ===\n",
    "features = [\n",
    "    'forward_phred',\n",
    "    'reverse_phred',\n",
    "    'normalized_position_forward',\n",
    "    'normalized_position_reverse', \n",
    "    'homopolymer_error_risk_forward',\n",
    "    'homopolymer_error_risk_reverse',\n",
    "    'homopolymer_length_forward',\n",
    "    'homopolymer_length_reverse',\n",
    "    'GC_content_forward',\n",
    "    'GC_content_reverse',\n",
    "    'entropy_forward',\n",
    "    'entropy_reverse',\n",
    "    'is_overlap',\n",
    "    'has_GCC_prev3_forward',\n",
    "    'has_GCC_prev3_reverse',\n",
    "    'window_abundance',\n",
    "    'global_abundance',\n",
    "    'local_abundance'\n",
    "]\n",
    "\n",
    "# === Step 3: Load validation data ===\n",
    "df = pd.read_csv(feature_file)\n",
    "\n",
    "# Get ground truth match letters\n",
    "true_labels = df[\"match_letter\"].replace(\"i\", \"s\").values\n",
    "\n",
    "# Prepare features for prediction\n",
    "X_val = df[features]\n",
    "X_val_imputed = pd.DataFrame(imputer.transform(X_val), columns=features)\n",
    "\n",
    "# === Step 4: Model predictions ===\n",
    "rf_proba = rf.predict_proba(X_val_imputed)[:, 1]\n",
    "xgb_proba = xgb.predict_proba(X_val_imputed)[:, 1]\n",
    "avg_proba = (rf_proba + xgb_proba) / 2\n",
    "\n",
    "# Apply voting threshold\n",
    "y_pred = (avg_proba >= threshold).astype(int)\n",
    "predicted_labels = np.where(y_pred == 0, 'm', 's')\n",
    "\n",
    "# === Step 5: Save for comparison ===\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"match_letter\": true_labels,\n",
    "    \"predicted_label\": predicted_labels\n",
    "})\n",
    "\n",
    "comparison_df.to_csv(output_file, index=False)\n",
    "print(f\"✅ Saved prediction vs match_letter comparison to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e93566",
   "metadata": {},
   "source": [
    "# Compare Results of New Mock Classification to its Correct Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Malak\\iped\\validation\\validation_predictions_for_comparison.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure expected columns exist\n",
    "assert 'match_letter' in df.columns and 'predicted_label' in df.columns, \"Missing required columns.\"\n",
    "\n",
    "# Step 1: Unique values\n",
    "true_unique = df['match_letter'].unique()\n",
    "pred_unique = df['predicted_label'].unique()\n",
    "\n",
    "# Step 2: Counts\n",
    "true_counts = df['match_letter'].value_counts()\n",
    "pred_counts = df['predicted_label'].value_counts()\n",
    "\n",
    "# Step 3: Confusion matrix\n",
    "conf_matrix = pd.crosstab(df['match_letter'], df['predicted_label'], rownames=['Actual'], colnames=['Predicted'], dropna=False)\n",
    "\n",
    "# Step 4: Metrics\n",
    "y_true = df['match_letter'].map({'m': 0, 's': 1}).values\n",
    "y_pred = df['predicted_label'].map({'m': 0, 's': 1}).values\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0  # Recall for class 's'\n",
    "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0  # Recall for class 'm'\n",
    "\n",
    "# Display\n",
    "print(\"✅ Unique values in 'match_letter':\", true_unique)\n",
    "print(\"✅ Unique values in 'predicted_label':\", pred_unique)\n",
    "print(\"\\n📊 Count of each class in 'match_letter':\\n\", true_counts)\n",
    "print(\"\\n📊 Count of each class in 'predicted_label':\\n\", pred_counts)\n",
    "print(\"\\n🧮 Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"\\n✅ Accuracy     : {accuracy:.4f}\")\n",
    "print(f\"✅ Sensitivity  : {sensitivity:.4f} (True 's' recall)\")\n",
    "print(f\"✅ Specificity  : {specificity:.4f} (True 'm' recall)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (venv)",
   "language": "python",
   "name": "py312_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
